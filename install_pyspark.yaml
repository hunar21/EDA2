- name: Install and Configure Spark and Java
  hosts: all
  become: true

  vars:
    spark_version: "3.5.1"
    hadoop_version: "hadoop3"
    spark_download_url: "https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz"
    spark_install_dir: "/opt/spark"

  tasks:
    - name: Install required packages
      yum:
        name:
          - java-11-openjdk-devel
          - wget
        state: present

    - name: Verify Java installation
      command: java -version
      ignore_errors: true

    - name: Download Spark
      get_url:
        url: "{{ spark_download_url }}"
        dest: "/tmp/spark-{{ spark_version }}-bin-{{ hadoop_version }}.tgz"
      register: spark_download

    - name: Extract Spark
      unarchive:
        src: "/tmp/spark-{{ spark_version }}-bin-{{ hadoop_version }}.tgz"
        dest: "/opt/"
        remote_src: yes
      when: spark_download.changed

    - name: Check if Spark symlink already exists
      stat:
        path: "{{ spark_install_dir }}"
      register: spark_symlink

    - name: Set Spark symlink
      file:
        src: "/opt/spark-{{ spark_version }}-bin-{{ hadoop_version }}"
        dest: "{{ spark_install_dir }}"
        state: link
      when: not spark_symlink.stat.exists

    - name: Set environment variables for Java and Spark
      copy:
        dest: /etc/profile.d/spark.sh
        content: |
          export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
          export SPARK_HOME={{ spark_install_dir }}
          export PATH=$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH

    - name: Source environment variables
      shell: source /etc/profile.d/spark.sh
      args:
        executable: /bin/bash

    - name: Verify Spark installation
      command: "{{ spark_install_dir }}/bin/spark-submit --version"
      ignore_errors: true

    - name: Start Spark Master
      command: "{{ spark_install_dir }}/sbin/start-master.sh"
      when: inventory_hostname in groups['hosts']
      ignore_errors: true

    - name: Start Spark Worker
      command: "bash -c 'source /etc/profile.d/spark.sh && {{ spark_install_dir }}/sbin/start-worker.sh spark://{{ groups['hosts'][0] }}:7077'"
      when: inventory_hostname in groups['workers']
      ignore_errors: true

    - name: Persist PYSPARK_PYTHON path for all users
      copy:
        dest: /etc/profile.d/pyspark_python.sh
        content: |
          export PYSPARK_PYTHON=/data/local/venv/bin/python
        mode: '0755'

    # ---------------------------------------------------------------------------
    # Option 2: Open a Broad Range of Ephemeral Ports on the Master Node
    # These tasks run only on nodes in the 'hosts' group (the master).
    # ---------------------------------------------------------------------------
    - name: Install firewalld on Master Node
      ansible.builtin.yum:
        name: firewalld
        state: present
      when: ansible_facts['os_family'] == "RedHat" and inventory_hostname in groups['hosts']

    - name: Start and enable firewalld on Master Node
      ansible.builtin.service:
        name: firewalld
        state: started
        enabled: yes
      when: ansible_facts['os_family'] == "RedHat" and inventory_hostname in groups['hosts']

    - name: Allow Spark master port 7077 via firewalld on Master Node
      ansible.posix.firewalld:
        port: 7077/tcp
        permanent: yes
        state: enabled
        immediate: yes
      when: ansible_facts['os_family'] == "RedHat" and inventory_hostname in groups['hosts']

    - name: Allow Spark ephemeral port range via firewalld on Master Node
      ansible.posix.firewalld:
        port: "32768-61000/tcp"
        permanent: yes
        state: enabled
        immediate: yes
      when: ansible_facts['os_family'] == "RedHat" and inventory_hostname in groups['hosts']

    - name: Reload firewalld configuration on Master Node
      ansible.builtin.command: firewall-cmd --reload
      when: ansible_facts['os_family'] == "RedHat" and inventory_hostname in groups['hosts']
